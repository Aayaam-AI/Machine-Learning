{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red15\green118\blue174;\red214\green215\blue217;
}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c54118\c73725;\cssrgb\c87059\c87451\c87843\c14902;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\fs48 \cf0 \cb2 Gradient Boosting{\field{\*\fldinst{HYPERLINK "https://www.kaggle.com/alexisbcook/xgboost#Gradient-Boosting"}}{\fldrslt \cf3 \'b6}}\cb1 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf0 \cb2 Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.\cb1 \
\cb2 It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)\cb1 \
\cb2 Then, we start the cycle:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.\cb1 \
\ls1\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
These predictions are used to calculate a loss function (like {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Mean_squared_error"}}{\fldrslt \cf3 mean squared error}}, for instance).\cb1 \
\ls1\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (
\i Side note: The "gradient" in "gradient boosting" refers to the fact that we'll use {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Gradient_descent"}}{\fldrslt \cf3 gradient descent}} on the loss function to determine the parameters in this new model.
\i0 )\cb1 \
\ls1\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Finally, we add the new model to ensemble, and ...\cb1 \
\ls1\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
... repeat!\
\pard\tx566\pardeftab720\partightenfactor0
\cf0 \

\fs48 Parameters-\
\pard\pardeftab720\partightenfactor0

\fs28 \cf0 \cb4  
\fs36 n_estimators
\fs28  - \cb2  specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf0 \cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Too 
\i low
\i0  a value causes 
\i underfitting
\i0 , which leads to inaccurate predictions on both training data and test data.\cb1 \
\ls2\ilvl0\cb2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Too 
\i high
\i0  a value causes 
\i overfitting
\i0 , which causes accurate predictions on training data, but inaccurate predictions on test data (
\i which is what we care about
\i0 ).\
\pard\pardeftab720\partightenfactor0

\fs32 \cf0 \cb1 \
\pard\pardeftab720\partightenfactor0

\fs36 \cf0 \cb4 early_stopping_rounds
\fs28  - \cb2  offers a way to automatically find the ideal value for \cb4 n_estimators\cb2 . Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for \cb4 n_estimators\cb2 . It's smart to set a high value for \cb4 n_estimators\cb2  and then use \cb4 early_stopping_rounds\cb2  to find the optimal time to stop iterating.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb2 Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting \cb4 early_stopping_rounds=5\cb2  is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores.\cb1 \
\cb2 When using \cb4 early_stopping_rounds\cb2 , you also need to set aside some data for calculating the validation scores - this is done by setting the \cb4 eval_set\cb2  parameter.\cb1 \
\pard\tx566\pardeftab720\partightenfactor0
\cf0 \cb2 \
\pard\tx566\pardeftab720\partightenfactor0

\fs36 \cf0 n_jobs - 
\fs28 On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter \cb4 n_jobs\cb2  equal to the number of cores on your machine. \
}